{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(cascade_path, filepath, filename, n = 200):\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    def crop_image(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        for (x, y, w, h) in faces:\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "        return cropped_face\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    id = 1\n",
    "    img_id = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if crop_image(frame) is not None:\n",
    "            img_id += 1\n",
    "            face = cv2.resize(crop_image(frame), (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            file_name_path = f\"{filepath}\\\\{filename}.\" + str(id) + \".\" + str(img_id) + \".jpg\"\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "            \n",
    "            cv2.imshow(\"Captured Face\", face)\n",
    "            if cv2.waitKey(1) == 13 or int(img_id) == n:\n",
    "                break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('Collecting images completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_path = r'C:\\Users\\user\\Downloads\\4.1\\haarcascade_frontalface_default.xml'\n",
    "happy = r\"C:\\Users\\user\\Downloads\\4.1\\MOOD\\me\\happy\"\n",
    "sad = r\"C:\\Users\\user\\Downloads\\4.1\\MOOD\\me\\sad\"\n",
    "neutral = r\"C:\\Users\\user\\Downloads\\4.1\\MOOD\\me\\neutral\"\n",
    "angry = r\"C:\\Users\\user\\Downloads\\4.1\\MOOD\\me\\angry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images completed...\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(cascade_path, happy, \"happy\", n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images completed...\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(cascade_path, sad, \"sad\", n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images completed...\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(cascade_path, neutral, \"neutral\", n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images completed...\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(cascade_path, angry, \"angry\", n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_folder):\n",
    "    faces = []\n",
    "    labels = []\n",
    "\n",
    "    emotions = {\n",
    "        \"happy\": 0,\n",
    "        \"sad\": 1,\n",
    "        \"neutral\": 2,\n",
    "\t    \"angry\": 3\n",
    "    }\n",
    "\n",
    "    for emotion in emotions:\n",
    "        emotion_folder = os.path.join(data_folder, emotion)\n",
    "        for image_name in os.listdir(emotion_folder):\n",
    "            image_path = os.path.join(emotion_folder, image_name)\n",
    "            face = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            face = cv2.resize(face, (200, 200))\n",
    "            faces.append(face)\n",
    "            labels.append(emotions[emotion])\n",
    "\n",
    "    return faces, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r\"C:\\Users\\user\\Downloads\\4.1\\MOOD\\me\"  \n",
    "faces, labels = prepare_training_data(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.array(faces)\n",
    "labels = to_categorical(labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = faces.reshape(-1, 200, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces, labels, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\conda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (4, 4), activation='relu', input_shape=(200, 200, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(4, activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 478ms/step - accuracy: 0.3094 - loss: 230.9060 - val_accuracy: 0.6875 - val_loss: 0.7512\n",
      "Epoch 2/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 507ms/step - accuracy: 0.7878 - loss: 0.5377 - val_accuracy: 0.6875 - val_loss: 0.9279\n",
      "Epoch 3/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 491ms/step - accuracy: 0.7191 - loss: 1.2355 - val_accuracy: 0.8375 - val_loss: 0.4084\n",
      "Epoch 4/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 477ms/step - accuracy: 0.8791 - loss: 0.3108 - val_accuracy: 0.9375 - val_loss: 0.2621\n",
      "Epoch 5/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 485ms/step - accuracy: 0.9476 - loss: 0.1599 - val_accuracy: 0.9125 - val_loss: 0.2691\n",
      "Epoch 6/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 454ms/step - accuracy: 0.9342 - loss: 0.2588 - val_accuracy: 0.8500 - val_loss: 0.4186\n",
      "Epoch 7/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 452ms/step - accuracy: 0.8482 - loss: 0.5808 - val_accuracy: 0.9125 - val_loss: 0.3931\n",
      "Epoch 8/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 454ms/step - accuracy: 0.8986 - loss: 0.2944 - val_accuracy: 0.9750 - val_loss: 0.2365\n",
      "Epoch 9/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 458ms/step - accuracy: 0.9701 - loss: 0.1338 - val_accuracy: 0.9750 - val_loss: 0.2043\n",
      "Epoch 10/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 464ms/step - accuracy: 0.9652 - loss: 0.1176 - val_accuracy: 0.9625 - val_loss: 0.3750\n",
      "Epoch 11/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 475ms/step - accuracy: 0.9820 - loss: 0.1045 - val_accuracy: 0.9750 - val_loss: 0.1939\n",
      "Epoch 12/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 517ms/step - accuracy: 0.9842 - loss: 0.0618 - val_accuracy: 0.9625 - val_loss: 0.2391\n",
      "Epoch 13/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.9899 - loss: 0.0375 - val_accuracy: 0.9625 - val_loss: 0.2436\n",
      "Epoch 14/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 475ms/step - accuracy: 0.9933 - loss: 0.0264 - val_accuracy: 0.9750 - val_loss: 0.2318\n",
      "Epoch 15/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 476ms/step - accuracy: 0.9910 - loss: 0.0278 - val_accuracy: 0.9625 - val_loss: 0.3284\n",
      "Epoch 16/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 473ms/step - accuracy: 0.9878 - loss: 0.0319 - val_accuracy: 0.9250 - val_loss: 0.2053\n",
      "Epoch 17/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 496ms/step - accuracy: 0.9763 - loss: 0.0721 - val_accuracy: 0.9750 - val_loss: 0.3191\n",
      "Epoch 18/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 484ms/step - accuracy: 0.9961 - loss: 0.0337 - val_accuracy: 0.9750 - val_loss: 0.3583\n",
      "Epoch 19/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 465ms/step - accuracy: 0.9760 - loss: 0.0663 - val_accuracy: 0.8625 - val_loss: 0.4635\n",
      "Epoch 20/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 504ms/step - accuracy: 0.9246 - loss: 0.4134 - val_accuracy: 0.9125 - val_loss: 0.4019\n",
      "Epoch 21/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 470ms/step - accuracy: 0.9549 - loss: 0.1625 - val_accuracy: 0.9125 - val_loss: 0.3027\n",
      "Epoch 22/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 458ms/step - accuracy: 0.9696 - loss: 0.1515 - val_accuracy: 0.9375 - val_loss: 0.2974\n",
      "Epoch 23/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 454ms/step - accuracy: 0.9796 - loss: 0.0942 - val_accuracy: 0.9625 - val_loss: 0.2130\n",
      "Epoch 24/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 457ms/step - accuracy: 0.9927 - loss: 0.0298 - val_accuracy: 0.9625 - val_loss: 0.2958\n",
      "Epoch 25/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 459ms/step - accuracy: 0.9803 - loss: 0.0606 - val_accuracy: 0.9750 - val_loss: 0.3374\n",
      "Epoch 26/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 470ms/step - accuracy: 0.9879 - loss: 0.0394 - val_accuracy: 0.9625 - val_loss: 0.2739\n",
      "Epoch 27/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 470ms/step - accuracy: 0.9923 - loss: 0.0375 - val_accuracy: 0.9500 - val_loss: 0.3195\n",
      "Epoch 28/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 457ms/step - accuracy: 0.9919 - loss: 0.0185 - val_accuracy: 0.9375 - val_loss: 0.2764\n",
      "Epoch 29/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 455ms/step - accuracy: 0.9602 - loss: 0.1862 - val_accuracy: 0.8500 - val_loss: 0.8853\n",
      "Epoch 30/30\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 457ms/step - accuracy: 0.8917 - loss: 0.3706 - val_accuracy: 0.8125 - val_loss: 0.5971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x163799a0190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in i:\\conda\\lib\\site-packages (from opencv-contrib-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_rec():\n",
    "  names = ['Unknown', 'neyra']\n",
    "  model = cv2.face.LBPHFaceRecognizer_create()\n",
    "  model.read('haarcascade_frontalface_default.xml')\n",
    "  camera = cv2.VideoCapture(0)\n",
    "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "  while True:\n",
    "    ret, img = camera.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "      cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "      gray = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
    "      roi = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "      try:\n",
    "        params = model.predict(roi)\n",
    "        label = names[params[0]]\n",
    "        print(label)\n",
    "        cv2.putText(img, label + \", \" + str(params[1]), (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "    cv2.imshow(\"camera\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"0\"):\n",
    "      break\n",
    "\n",
    "  camera.release()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  face_rec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From i:\\conda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI (Region of Interest)\n",
    "        face_roi = rgb_frame[y:y + h, x:x + w]\n",
    "\n",
    "        \n",
    "        # Perform emotion analysis on the face ROI\n",
    "        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0]['dominant_emotion']\n",
    "\n",
    "        # Draw rectangle around face and label with predicted emotion\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('MOOD', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('0'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
